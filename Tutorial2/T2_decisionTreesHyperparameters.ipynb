{"cells":[{"cell_type":"markdown","metadata":{"id":"MPjgtz3IMDqX"},"source":["Estructura de la sesión:\n","- Ajuste de hiperparámetros en los árboles de decisión\n","- Combinar el ajuste de hiperparámetros y la evaluación de modelos\n","  - GridSearch\n","  - RandomSearch\n","  - Optimización basada en modelos\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RjO0ZHCkZp30"},"source":["# HIPERPARÁMETROS DE LOS ÁRBOLES DE DECISIÓN. AJUSTE DE ÁRBOLES DE DECISIÓN"]},{"cell_type":"markdown","metadata":{"id":"k_5O3DQkZp31"},"source":["- **max_depth : int or None, opcional (por defecto=None)**\n","    La profundidad máxima del árbol. Si es None, los nodos se expanden hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos muestras que min_samples_split. Se ignora si max_leaf_nodes no es None.\n","    \n","- **min_samples_split : int, opcional (por defecto=2)**\n","    El número mínimo de muestras necesarias para dividir un nodo interno.\n","\n","- Hay más parámetros:\n","  - https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n","  - https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5tfYm7QAZp32"},"source":["Primero, se cargan los datos, las entradas van a X, las salidas a y."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJD421uMZp33"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.datasets import fetch_california_housing\n","from scipy.stats import sem\n","\n","housing_meta = fetch_california_housing()\n","X = housing_meta.data\n","y = housing_meta.target\n","\n","print(housing_meta.DESCR)"]},{"cell_type":"markdown","metadata":{"id":"XQOtMG7hZp35"},"source":["## COMBINAR EL AJUSTE DE HIPERPARÁMETROS Y LA EVALUACIÓN DE MODELOS"]},{"cell_type":"markdown","metadata":{"id":"2Ry14avEZp36"},"source":["La combinación de evaluación de modelos y ajuste de hiperparámetros puede entenderse como un bucle externo (outer) que entrena un modelo y prueba el modelo, y un bucle interno (inner), donde el proceso de entrenamiento consiste en buscar los mejores hiperparámetros, y luego obtener el modelo con esos mejores hiperparámetros.\n","\n","En primer lugar, vamos a utilizar **Holdout** (entrenar/probar) para la evaluación del modelo (bucle externo o **outer**), y **3-fold crossvalidation** para el ajuste de los hiperparámetros (bucle interno o **inner**). Los hiperparámetros se ajustarán con **Gridsearch**."]},{"cell_type":"markdown","metadata":{"id":"GQQaJrViZp37"},"source":["#### GRIDSEARCH"]},{"cell_type":"markdown","metadata":{"id":"7rqhsc9OARd3"},"source":["En primer lugar, definamos nuestra función python para el RMSE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TM7SzJLHAQhZ"},"outputs":[],"source":["def rmse(y_test, y_test_pred):\n","  \"\"\" This is my computation of Root Mean Squared Error \"\"\"\n","  return np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"laxLj26PZp37"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Holdout for model evaluation. 33% of available data for test\n","X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"9-SI_kTr82RI"},"source":["En primer lugar, recordemos el RMSE con hiperparámetros por defecto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rU6g-6PZ87DS"},"outputs":[],"source":["from sklearn import metrics\n","from sklearn.tree import DecisionTreeRegressor\n","\n","regr = DecisionTreeRegressor()\n","np.random.seed(42)\n","regr.fit(X=X_train, y=y_train)\n","print(f\"RMSE of tree with default hyper-pars: {rmse(y_test, regr.predict(X=X_test))}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRknuZjUZp39"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV, KFold\n","from sklearn.linear_model import LogisticRegression\n","#from sklearn.model_selection import KFold\n","\n","\n","# Search space\n","\n","param_grid = {'max_depth': list(range(2,16,2)),\n","              'min_samples_split': list(range(2,16,2))}\n","\n","inner = KFold(n_splits=3, shuffle=True, random_state=42)\n","\n","# Definition of a 2-step process that self-adjusts 2 hyperpars\n","regr = GridSearchCV(DecisionTreeRegressor(),\n","                   param_grid,\n","                   scoring='neg_mean_squared_error',\n","                   cv=inner,\n","                   n_jobs=1, verbose=1)\n","\n","# Train the self-adjusting process\n","np.random.seed(42)\n","regr.fit(X=X_train, y=y_train)\n","\n","# At this point, regr contains the model with the best hyper-parameters found by gridsearch\n","# and trained on the complete X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZokHzT78udT"},"outputs":[],"source":["# Now, the performance of regr is computed on the test partition\n","\n","print(f\"RMSE of tree with hyper-parameter tuning (grid-search): {rmse(y_test, regr.predict(X=X_test))}\")"]},{"cell_type":"markdown","metadata":{"id":"dTTNB93mZp4A"},"source":["Veamos los mejores hiperparámetros y su puntuación (MSE). Podemos ver que están cerca de los extremos del espacio de parámetros, pero no en el extremo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVeTEIFaZp4B"},"outputs":[],"source":["regr.best_params_, -regr.best_score_"]},{"cell_type":"markdown","metadata":{"id":"Hk6rBqZSZp4I"},"source":["#### BUSQUEDA ALEATORIA (RANDOMIZED SEARCH)"]},{"cell_type":"markdown","metadata":{"id":"w7bZU45-Zp4I"},"source":["Ahora, utilicemos **Búsqueda aleatoria** en lugar de gridsearch. Sólo se probarán 20 combinaciones de valores de hiperparámetros (budget=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urjn49EAZp4J"},"outputs":[],"source":["from sklearn.model_selection import RandomizedSearchCV, KFold\n","from sklearn import metrics\n","\n","# Search space\n","param_grid = {'max_depth': list(range(2,16,2)),\n","              'min_samples_split': list(range(2,16,2))}\n","\n","# Inner evaluation\n","inner = KFold(n_splits=3, shuffle=True, random_state=42)\n","\n","budget = 20\n","regr = RandomizedSearchCV(DecisionTreeRegressor(),\n","                         param_grid,\n","                         scoring='neg_mean_squared_error',\n","                         cv=inner,\n","                         n_jobs=1, verbose=1,\n","                         n_iter=budget\n","                        )\n","np.random.seed(42)\n","regr.fit(X=X_train, y=y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2JJyJAn-6Ci"},"outputs":[],"source":["# Now, the performance of regr is computed on the test partition\n","\n","print(f\"RMSE of tree with hyper-parameter tuning (random search): {rmse(y_test, regr.predict(X=X_test))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OzNtlMYZp4L"},"outputs":[],"source":["regr.best_params_, -regr.best_score_"]},{"cell_type":"markdown","metadata":{"id":"Nk1-fYyoZp4N"},"source":["Para la **Búsqueda Aleatoria**, podemos definir el espacio de búsqueda con distribuciones estadísticas, en lugar de utilizar valores particulares como hacíamos antes. A continuación puedes ver cómo utilizar una distribución uniforme sobre enteros entre 2 y 16 mediante *randint*. Para hiperparámetros continuos podríamos usar distribuciones continuas como *uniform* o *expon* (exponencial)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vyv0cvAZp4O"},"outputs":[],"source":["from sklearn.model_selection import RandomizedSearchCV, KFold\n","from sklearn import metrics\n","\n","\n","from scipy.stats import uniform, expon\n","from scipy.stats import randint as sp_randint\n","\n","# Search space with integer uniform distributions\n","param_grid = {'max_depth': sp_randint(2,16),\n","              'min_samples_split': sp_randint(2,16)}\n","\n","inner = KFold(n_splits=3, shuffle=True, random_state=42)\n","\n","budget = 20\n","regr = RandomizedSearchCV(DecisionTreeRegressor(),\n","                         param_grid,\n","                         scoring='neg_mean_squared_error',\n","                         cv=inner,\n","                         n_jobs=1, verbose=1,\n","                         n_iter=budget\n","                        )\n","\n","np.random.seed(42)\n","regr.fit(X=X_train, y=y_train)\n","\n","# At this point, regr contains the model with the best hyper-parameters found by gridsearch\n","# and trained on the complete X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRppQBOiA9fI"},"outputs":[],"source":["# Now, the performance of regr is computed on the test partition\n","\n","print(f\"RMSE of tree with hyper-parameter tuning (random search II): {rmse(y_test, regr.predict(X=X_test))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhMPDyL8Zp4Q"},"outputs":[],"source":["regr.best_params_, -regr.best_score_"]},{"cell_type":"markdown","metadata":{"id":"je3hVWP9Zp4S"},"source":["¿Y si quisiéramos hacer **evaluación de modelos con 5-fold crossvalidation** y **ajuste de hiperparámetros con 3-fold crossvalidation**? Esto se denomina validación cruzada anidada https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html). Hay un bucle externo (para evaluar los modelos) y un bucle interno (para ajustar los hiperparámetros)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkJD1ifoZp4S"},"outputs":[],"source":["from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n","from sklearn import metrics\n","\n","# random_state=42 for reproducibility\n","# Evaluation of model (outer loop)\n","outer = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","\n","from scipy.stats import uniform, expon\n","\n","# Search space\n","param_grid = {'max_depth': list(range(2,16,2)),\n","              'min_samples_split': list(range(2,16,2))}\n","\n","inner = KFold(n_splits=3, shuffle=True, random_state=42)\n","\n","budget = 20\n","# This is the internal 3-fold crossvalidation for hyper-parameter tuning\n","regr = RandomizedSearchCV(DecisionTreeRegressor(),\n","                         param_grid,\n","                         scoring='neg_mean_squared_error',\n","                         # 3-fold for hyper-parameter tuning\n","                         cv=inner,\n","                         n_jobs=1, verbose=1,\n","                         n_iter=budget\n","                        )\n","\n","# This is the external 5-fold crossvalidation for model evaluation\n","# Notice that regr is the model resulting of hyper-parameter tuning\n","np.random.seed(42)\n","\n","# For sklearn, higher scores are better. Given that MSE is an error (smaller is better), the corresponding score is -MSE\n","scores = -cross_val_score(regr,\n","                            X, y,\n","                            scoring='neg_mean_squared_error',\n","                            cv = outer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxeEDgv1Zp4U"},"outputs":[],"source":["print(scores)\n","# The score was MSE, we want RMSE\n","scores = np.sqrt(scores)\n","# The mean of the 5-fold crossvalidation is the final score of the model\n","print(f\"{scores.mean()} +- {scores.std()}\")"]},{"cell_type":"markdown","metadata":{"id":"LdfqhEw9Zp4W"},"source":["#### OBTENCIÓN DEL MODELO FINAL (PARA SU DESPLIEGUE, O PARA ENVIARLO A UN CONCURSO, ...)"]},{"cell_type":"markdown","metadata":{"id":"O5w47vRHZp4X"},"source":["Si necesitamos un modelo final, podemos obtenerlo ajustando regr a todos los datos disponibles. Recordemos que regr hace ajuste de hiper-parámetros."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Kvg0RX-Zp4X"},"outputs":[],"source":["np.random.seed(42)\n","\n","# Fitting again the randomized search HPO\n","regrFinal = regr.fit(X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRfsLFLTZp4Z"},"outputs":[],"source":["regr.best_params_, -regr.best_score_"]},{"cell_type":"markdown","metadata":{"id":"2XeQqmjrZp4e"},"source":["### OPTIMIZACIÓN BASADA EN MODELOS (BAYESIAN OPTIMIZATION)"]},{"cell_type":"markdown","metadata":{"id":"YNwHbEIsZp4f"},"source":["Para ello se utilizará scikit-optimize (skopt): https://scikit-optimize.github.io. **Holdout** para la evaluación del modelo y **3-fold crossvalidation** para el ajuste de hiperparámetros (con **Model Based Optimization** )."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45cWJKARCT2Y"},"outputs":[],"source":["# !pip install scikit-learn==0.24.2\n","# !pip install scikit-optimize\n","!pip install git+https://github.com/scikit-optimize/scikit-optimize.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DkKc_paMZ-Oh"},"outputs":[],"source":["# Checking that everything is correct with skopt (0.9.dev0) and sklearn\n","from skopt import __version__\n","print(__version__)\n","from sklearn import __version__\n","print(__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmxEk_w4cEb7"},"outputs":[],"source":["from skopt import BayesSearchCV\n","from skopt.space import Integer, Real, Categorical\n","from sklearn import metrics\n","from skopt.plots import plot_objective, plot_histogram, plot_convergence\n","from sklearn.model_selection import KFold\n","\n","# Search space with integer uniform distributions\n","param_grid = {'max_depth': Integer(2,20),\n","              'min_samples_split': Integer(20,100)}\n","\n","inner = KFold(n_splits=3, shuffle=True, random_state=42)\n","budget = 20\n","regr = BayesSearchCV(DecisionTreeRegressor(),\n","                    param_grid,\n","                    scoring='neg_mean_squared_error',\n","                    cv=inner,\n","                    n_jobs=1, verbose=1,\n","                    n_iter=budget\n","                    )\n","np.random.seed(42)\n","regr.fit(X=X_train, y=y_train)\n","\n","# At this point, regr contains the model with the best hyper-parameters found by bayessearch\n","# and trained on the complete X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIqeUTQ6Zp4k"},"outputs":[],"source":["# Now, the performance of regr is computed on the test partition\n","\n","print(f\"RMSE of tree with hyper-parameter tuning (bayesian optimization): {rmse(y_test, regr.predict(X=X_test))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlZ321QBZp4l"},"outputs":[],"source":["regr.best_params_, -regr.best_score_"]},{"cell_type":"markdown","metadata":{"id":"2YSNFqSaF6Qx"},"source":["Podemos comprobar si la optimización ha convergido"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjW8T92qFu7f"},"outputs":[],"source":["_ = plot_convergence(regr.optimizer_results_[0])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yEHkN0ECMJ-"},"outputs":[],"source":["_ = plot_objective(regr.optimizer_results_[0],\n","                   dimensions=['max_depth', 'min_samples_split'],\n","                   n_minimum_search=int(1e8))\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}